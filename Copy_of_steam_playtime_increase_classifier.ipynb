{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5_lTROUh62U"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# =============================================================================\n",
        "# Project description (high-level summary for report / final project)\n",
        "# =============================================================================\n",
        "# This script trains a supervised learning model to predict players'\n",
        "# post-review engagement (future playtime) on Steam using review-time features.\n",
        "#\n",
        "# Data:\n",
        "#   - Three Steam review datasets for specific games, stored as Excel files:\n",
        "#       /content/Cyberpunk 2077_steam_reviews_1091500.part000_partial (1).xlsx\n",
        "#       /content/Red Dead Redemption2_steam_reviews_1174180.part000_with_sent (1).xlsx\n",
        "#       /content/Witcher 3_steam_reviews_292030.part000_with_sent (1).xlsx\n",
        "#\n",
        "# Target variable:\n",
        "#   - author_playtime_forever: total playtime in minutes (after the review)\n",
        "#   - author_playtime_at_review: playtime in minutes at the moment of the review\n",
        "#   - We define:\n",
        "#       delta_playtime = author_playtime_forever - author_playtime_at_review\n",
        "#     which measures how much additional playtime the player accumulates after\n",
        "#     posting the review (future engagement).\n",
        "#\n",
        "# Main modeling choices:\n",
        "#   1. We remove rows with missing or negative delta_playtime.\n",
        "#   2. We clip delta_playtime at the 99th percentile to reduce the influence\n",
        "#      of extreme \"grinder\" players with thousands of hours.\n",
        "#   3. We apply a log1p transformation to the clipped target:\n",
        "#         delta_log = log1p(delta_playtime_clipped)\n",
        "#      and train the model in log space for more stable optimization.\n",
        "#   4. We use a HistGradientBoostingRegressor with loss='absolute_error'\n",
        "#      which approximately optimizes the Median Absolute Error (MAE) in the\n",
        "#      transformed space.\n",
        "#   5. We split the data into train / validation / test sets using\n",
        "#      GroupShuffleSplit by author_steamid, so that the same player\n",
        "#      does not appear in both training and test sets (prevents leakage).\n",
        "#   6. Input features include:\n",
        "#        - numeric player and review statistics (playtime at review,\n",
        "#          last two weeks' playtime, votes_up, votes_funny, etc.),\n",
        "#        - binary flags (voted_up, steam_purchase, etc.),\n",
        "#        - textual / lexical features (length, specificity, etc., if present),\n",
        "#        - sentiment label and sentiment score (if available),\n",
        "#        - language of the review.\n",
        "#\n",
        "# Outputs:\n",
        "#   - The script prints MAE and RMSE (in minutes) on:\n",
        "#       - Training set\n",
        "#       - Validation set\n",
        "#       - Test set\n",
        "#   - It also prints baseline MAE/RMSE when we simply predict:\n",
        "#       (a) the mean of delta_playtime_clipped on the test set\n",
        "#       (b) the median of delta_playtime_clipped on the test set\n",
        "#   - Finally, it saves a CSV file with test-set rows and model predictions:\n",
        "#       /content/test_predictions_delta_playtime_econ_log_clip_mae.csv\n",
        "#\n",
        "# How to interpret the results:\n",
        "#   - MAE (Mean Absolute Error) in minutes:\n",
        "#       On average, the absolute difference between the predicted future\n",
        "#       playtime (delta_playtime) and the true future playtime. For example,\n",
        "#       MAE = 2000 minutes means the model is off by about\n",
        "#       2000 / 60 ≈ 33 hours on average.\n",
        "#   - RMSE (Root Mean Squared Error) in minutes:\n",
        "#       More sensitive to large errors; a higher RMSE relative to MAE\n",
        "#       indicates heavy tails and some very large mistakes on extreme users.\n",
        "#   - If the model's MAE is significantly lower than the median-baseline MAE,\n",
        "#     it means the features (playtime at review, review text characteristics,\n",
        "#     sentiment, etc.) contain meaningful predictive signal about future\n",
        "#     engagement, beyond simply using the overall average.\n",
        "#\n",
        "# Business meaning:\n",
        "#   - This model can be interpreted as an \"engagement prediction\" model\n",
        "#     conditional on having written a review:\n",
        "#       given a player's playtime and their review at time t,\n",
        "#       how much more will they likely play this game afterward?\n",
        "#   - Platforms or developers could use such a model to:\n",
        "#       * Identify highly engaged players early,\n",
        "#       * Understand which types of reviews (length, sentiment, specificity)\n",
        "#         are associated with continued play,\n",
        "#       * Design interventions or personalization strategies for players\n",
        "#         predicted to churn (low future delta_playtime).\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 0. Config\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "# Excel files uploaded to /content in Colab\n",
        "DATA_FILES = [\n",
        "    \"/content/Cyberpunk 2077_steam_reviews_1091500.part000_partial (1).xlsx\",\n",
        "    \"/content/Red Dead Redemption2_steam_reviews_1174180.part000_with_sent (1).xlsx\",\n",
        "    \"/content/Witcher 3_steam_reviews_292030.part000_with_sent (1).xlsx\",\n",
        "]\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.2\n",
        "VAL_SIZE = 0.2  # fraction of (train+val) that will be used as validation\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1. Load and concatenate data\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "def load_all_files(file_list) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load all provided Excel files and concatenate them into a single DataFrame.\n",
        "    \"\"\"\n",
        "    all_dfs = []\n",
        "\n",
        "    for path in file_list:\n",
        "        if not os.path.exists(path):\n",
        "            raise FileNotFoundError(f\"File not found: {path}\")\n",
        "        print(f\"Loading XLSX: {path}\")\n",
        "        df = pd.read_excel(path)  # Colab has openpyxl support by default\n",
        "        all_dfs.append(df)\n",
        "\n",
        "    df_all = pd.concat(all_dfs, ignore_index=True)\n",
        "    print(f\"Total rows after concat: {len(df_all)}\")\n",
        "    return df_all\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2. Basic cleaning + target construction\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "def coalesce_column(df: pd.DataFrame, main_col: str, backup_cols):\n",
        "    \"\"\"\n",
        "    Ensure df[main_col] exists and fill its NaNs from backup_cols in order.\n",
        "    backup_cols: list of column names that may or may not exist.\n",
        "    \"\"\"\n",
        "    if main_col not in df.columns:\n",
        "        df[main_col] = np.nan\n",
        "\n",
        "    for bcol in backup_cols:\n",
        "        if bcol in df.columns:\n",
        "            df[main_col] = df[main_col].fillna(df[bcol])\n",
        "\n",
        "\n",
        "def prepare_dataset(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df_raw.copy()\n",
        "\n",
        "    # ---- 2.1 Coalesce key ID / language / playtime columns ----------------\n",
        "    coalesce_column(\n",
        "        df,\n",
        "        \"author_steamid\",\n",
        "        [\n",
        "            \"author_steamid_from_partial\",\n",
        "            \"author_steamid_from_partial_from_partial\",\n",
        "        ],\n",
        "    )\n",
        "    coalesce_column(\n",
        "        df,\n",
        "        \"author_language\",\n",
        "        [\n",
        "            \"author_language_from_partial\",\n",
        "            \"author_language_from_partial.1\",\n",
        "            \"author_language_from_partial_from_partial\",\n",
        "        ],\n",
        "    )\n",
        "    coalesce_column(\n",
        "        df,\n",
        "        \"author_playtime_at_review\",\n",
        "        [\n",
        "            \"author_playtime_at_review_from_partial\",\n",
        "            \"author_playtime_at_review_from_partial.1\",\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    # Ensure author_steamid exists and is usable for grouping\n",
        "    if \"author_steamid\" not in df.columns:\n",
        "        raise ValueError(\"Column 'author_steamid' is required but not found.\")\n",
        "    df[\"author_steamid\"] = df[\"author_steamid\"].astype(str).str.strip()\n",
        "\n",
        "    # Drop rows with missing author_steamid\n",
        "    before_auth = len(df)\n",
        "    df = df.replace({\"\": np.nan})\n",
        "    df = df.dropna(subset=[\"author_steamid\"])\n",
        "    after_auth = len(df)\n",
        "    print(f\"Dropped {before_auth - after_auth} rows with missing author_steamid; remaining: {after_auth}\")\n",
        "\n",
        "    # ---- 2.2 Numeric conversion for playtime columns ----------------------\n",
        "    for col in [\n",
        "        \"author_playtime_forever\",\n",
        "        \"author_playtime_at_review\",\n",
        "        \"author_playtime_last_two_weeks\",\n",
        "    ]:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "    # ---- 2.3 Target: delta_playtime ---------------------------------------\n",
        "    df[\"delta_playtime\"] = df[\"author_playtime_forever\"] - df[\"author_playtime_at_review\"]\n",
        "\n",
        "    before = len(df)\n",
        "    df = df.dropna(subset=[\"author_playtime_forever\", \"author_playtime_at_review\", \"delta_playtime\"])\n",
        "    df = df[df[\"delta_playtime\"] >= 0]\n",
        "    after = len(df)\n",
        "    print(f\"Dropped {before - after} rows with missing/negative target; remaining: {after}\")\n",
        "\n",
        "    # ---- 2.4 Clip extreme deltas at 99th percentile -----------------------\n",
        "    q99 = df[\"delta_playtime\"].quantile(0.99)\n",
        "    print(f\"Clipping delta_playtime at 99th percentile: {q99:.2f} minutes (~{q99/60:.1f} hours)\")\n",
        "    df[\"delta_playtime_clipped\"] = np.minimum(df[\"delta_playtime\"], q99)\n",
        "\n",
        "    # log1p target for modeling\n",
        "    df[\"delta_log\"] = np.log1p(df[\"delta_playtime_clipped\"])\n",
        "\n",
        "    # ---- 2.5 Boolean-like columns → 0/1 -----------------------------------\n",
        "    bool_cols = [\n",
        "        \"voted_up\",\n",
        "        \"steam_purchase\",\n",
        "        \"received_for_free\",\n",
        "        \"written_during_early_access\",\n",
        "        \"primarily_steam_deck\",\n",
        "    ]\n",
        "    for col in bool_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = (\n",
        "                df[col]\n",
        "                .replace(\n",
        "                    {\n",
        "                        \"TRUE\": 1,\n",
        "                        \"True\": 1,\n",
        "                        True: 1,\n",
        "                        \"FALSE\": 0,\n",
        "                        \"False\": 0,\n",
        "                        False: 0,\n",
        "                    }\n",
        "                )\n",
        "                .fillna(0)\n",
        "                .astype(int)\n",
        "            )\n",
        "        else:\n",
        "            df[col] = 0\n",
        "\n",
        "    # ---- 2.6 Sentiment & lexical features ---------------------------------\n",
        "    # These three files may not have sentiment_label / sentiment_score.\n",
        "    # We provide safe defaults and fill missing values.\n",
        "\n",
        "    if \"sentiment_label\" not in df.columns:\n",
        "        df[\"sentiment_label\"] = \"unknown\"\n",
        "    df[\"sentiment_label\"] = df[\"sentiment_label\"].astype(str).fillna(\"unknown\")\n",
        "\n",
        "    if \"sentiment_score\" in df.columns:\n",
        "        df[\"sentiment_score\"] = pd.to_numeric(df[\"sentiment_score\"], errors=\"coerce\").fillna(0.0)\n",
        "    else:\n",
        "        df[\"sentiment_score\"] = 0.0\n",
        "\n",
        "    # author_language\n",
        "    if \"author_language\" in df.columns:\n",
        "        df[\"author_language\"] = df[\"author_language\"].astype(str).fillna(\"unknown\")\n",
        "    else:\n",
        "        df[\"author_language\"] = \"unknown\"\n",
        "\n",
        "    # Numeric text features — used in the econ lexical dataset; safe to apply here too.\n",
        "    numeric_text_cols = [\n",
        "        \"len_char\",\n",
        "        \"len_token\",\n",
        "        \"R_len\",\n",
        "        \"R_digit\",\n",
        "        \"specificity_idf\",\n",
        "        \"R_spec\",\n",
        "        \"R_fam\",\n",
        "        \"R_conc\",\n",
        "        \"specificity_lex\",\n",
        "    ]\n",
        "    for col in numeric_text_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0.0)\n",
        "        else:\n",
        "            df[col] = 0.0\n",
        "\n",
        "    # ---- 2.7 Other numeric features for the model -------------------------\n",
        "    other_numeric_cols = [\n",
        "        \"author_num_games_owned\",\n",
        "        \"author_num_reviews\",\n",
        "        \"author_playtime_at_review\",\n",
        "        \"author_playtime_last_two_weeks\",\n",
        "        \"votes_up\",\n",
        "        \"votes_funny\",\n",
        "        \"comment_count\",\n",
        "        \"weighted_vote_score\",\n",
        "    ]\n",
        "    for col in other_numeric_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0.0)\n",
        "        else:\n",
        "            df[col] = 0.0\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3. Group-based train/val/test split by author_steamid\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "def group_train_val_test_split(\n",
        "    df: pd.DataFrame,\n",
        "    group_col: str = \"author_steamid\",\n",
        "    test_size: float = TEST_SIZE,\n",
        "    val_size: float = VAL_SIZE,\n",
        "    random_state: int = RANDOM_STATE,\n",
        "):\n",
        "    \"\"\"\n",
        "    Split the DataFrame into train / validation / test sets based on groups\n",
        "    (here: author_steamid), so that the same player does not appear in multiple\n",
        "    splits.\n",
        "    \"\"\"\n",
        "    groups = df[group_col].values\n",
        "\n",
        "    # 1) Train+val vs test\n",
        "    gss1 = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
        "    train_val_idx, test_idx = next(gss1.split(df, groups=groups))\n",
        "\n",
        "    df_train_val = df.iloc[train_val_idx].reset_index(drop=True)\n",
        "    df_test = df.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "    # 2) Train vs val inside train_val\n",
        "    groups_tv = df_train_val[group_col].values\n",
        "    gss2 = GroupShuffleSplit(n_splits=1, test_size=val_size, random_state=random_state + 1)\n",
        "    train_idx, val_idx = next(gss2.split(df_train_val, groups=groups_tv))\n",
        "\n",
        "    df_train = df_train_val.iloc[train_idx].reset_index(drop=True)\n",
        "    df_val = df_train_val.iloc[val_idx].reset_index(drop=True)\n",
        "\n",
        "    print(\n",
        "        f\"Train size: {len(df_train)}, Val size: {len(df_val)}, Test size: {len(df_test)}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"Unique authors - Train: {df_train[group_col].nunique()}, \"\n",
        "        f\"Val: {df_val[group_col].nunique()}, \"\n",
        "        f\"Test: {df_test[group_col].nunique()}\"\n",
        "    )\n",
        "\n",
        "    return df_train, df_val, df_test\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4. Build model pipeline\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "def build_model():\n",
        "    \"\"\"\n",
        "    Define the preprocessing and regression pipeline:\n",
        "      - Scale numeric features,\n",
        "      - Pass through boolean features,\n",
        "      - One-hot encode categorical features,\n",
        "      - Train HistGradientBoostingRegressor with absolute_error loss.\n",
        "    \"\"\"\n",
        "    # Continuous numeric features\n",
        "    numeric_features = [\n",
        "        \"author_num_games_owned\",\n",
        "        \"author_num_reviews\",\n",
        "        \"author_playtime_at_review\",\n",
        "        \"author_playtime_last_two_weeks\",\n",
        "        \"votes_up\",\n",
        "        \"votes_funny\",\n",
        "        \"comment_count\",\n",
        "        \"weighted_vote_score\",\n",
        "        \"len_char\",\n",
        "        \"len_token\",\n",
        "        \"R_len\",\n",
        "        \"R_digit\",\n",
        "        \"specificity_idf\",\n",
        "        \"R_spec\",\n",
        "        \"R_fam\",\n",
        "        \"R_conc\",\n",
        "        \"specificity_lex\",\n",
        "        \"sentiment_score\",\n",
        "    ]\n",
        "\n",
        "    # Boolean / 0-1 features\n",
        "    bool_features = [\n",
        "        \"voted_up\",\n",
        "        \"steam_purchase\",\n",
        "        \"received_for_free\",\n",
        "        \"written_during_early_access\",\n",
        "        \"primarily_steam_deck\",\n",
        "    ]\n",
        "\n",
        "    # Categorical features\n",
        "    categorical_features = [\n",
        "        \"author_language\",\n",
        "        \"sentiment_label\",\n",
        "    ]\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", StandardScaler(), numeric_features),\n",
        "            (\"bool\", \"passthrough\", bool_features),\n",
        "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    regressor = HistGradientBoostingRegressor(\n",
        "        loss=\"absolute_error\",\n",
        "        random_state=RANDOM_STATE,\n",
        "    )\n",
        "\n",
        "    model = Pipeline(\n",
        "        steps=[\n",
        "            (\"preprocess\", preprocessor),\n",
        "            (\"regressor\", regressor),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    feature_cols = numeric_features + bool_features + categorical_features\n",
        "    return model, feature_cols\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5. Train / evaluate\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "def evaluate_regression(y_true, y_pred, split_name: str):\n",
        "    \"\"\"\n",
        "    Print regression metrics:\n",
        "      - MAE: Mean Absolute Error (in minutes),\n",
        "      - RMSE: Root Mean Squared Error (in minutes).\n",
        "    This implementation is compatible with older sklearn versions by\n",
        "    manually taking the square root of MSE instead of using squared=False.\n",
        "    \"\"\"\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    print(f\"[{split_name}] MAE = {mae:.2f} minutes, RMSE = {rmse:.2f} minutes\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    # 1) Load all data files\n",
        "    df_raw = load_all_files(DATA_FILES)\n",
        "\n",
        "    # 2) Clean data and construct the target\n",
        "    df = prepare_dataset(df_raw)\n",
        "\n",
        "    # 3) Split by author_steamid (group-based)\n",
        "    df_train, df_val, df_test = group_train_val_test_split(df, group_col=\"author_steamid\")\n",
        "\n",
        "    # 4) Build model and feature list\n",
        "    model, feature_cols = build_model()\n",
        "\n",
        "    # 5) Prepare X / y  (target = delta_log)\n",
        "    X_train = df_train[feature_cols]\n",
        "    y_train = df_train[\"delta_log\"]\n",
        "\n",
        "    X_val = df_val[feature_cols]\n",
        "    y_val = df_val[\"delta_log\"]\n",
        "\n",
        "    X_test = df_test[feature_cols]\n",
        "    y_test = df_test[\"delta_log\"]\n",
        "\n",
        "    # 6) Fit the model\n",
        "    print(\"\\nFitting model (target = log1p(delta_playtime_clipped), loss=absolute_error) ...\\n\")\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # 7) Predict in log space and transform back to minutes\n",
        "    y_train_pred_log = model.predict(X_train)\n",
        "    y_val_pred_log = model.predict(X_val)\n",
        "    y_test_pred_log = model.predict(X_test)\n",
        "\n",
        "    y_train_pred = np.expm1(y_train_pred_log)\n",
        "    y_val_pred = np.expm1(y_val_pred_log)\n",
        "    y_test_pred = np.expm1(y_test_pred_log)\n",
        "\n",
        "    # True targets in minutes (clipped)\n",
        "    y_train_true = df_train[\"delta_playtime_clipped\"].values\n",
        "    y_val_true = df_val[\"delta_playtime_clipped\"].values\n",
        "    y_test_true = df_test[\"delta_playtime_clipped\"].values\n",
        "\n",
        "    # 8) Evaluation\n",
        "    print(\"Evaluation (on clipped delta_playtime, in minutes):\")\n",
        "    evaluate_regression(y_train_true, y_train_pred, \"Train\")\n",
        "    evaluate_regression(y_val_true, y_val_pred, \"Validation\")\n",
        "    evaluate_regression(y_test_true, y_test_pred, \"Test\")\n",
        "\n",
        "    # 9) Baselines on Test\n",
        "    print(\"\\nBaseline on Test (predicting raw delta_playtime_clipped):\")\n",
        "    mean_baseline = np.full_like(y_test_true, y_test_true.mean())\n",
        "    median_baseline = np.full_like(y_test_true, np.median(y_test_true))\n",
        "\n",
        "    print(\"[[Mean baseline]]\")\n",
        "    evaluate_regression(y_test_true, mean_baseline, \"Test (mean baseline)\")\n",
        "\n",
        "    print(\"[[Median baseline]]\")\n",
        "    evaluate_regression(y_test_true, median_baseline, \"Test (median baseline)\")\n",
        "\n",
        "    # 10) Save test predictions\n",
        "    df_test_out = df_test.copy()\n",
        "    df_test_out[\"delta_playtime_clipped_true\"] = y_test_true\n",
        "    df_test_out[\"delta_playtime_pred\"] = y_test_pred\n",
        "\n",
        "    out_path = \"/content/test_predictions_delta_playtime_econ_log_clip_mae.csv\"\n",
        "    df_test_out.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
        "    print(f\"\\nSaved test predictions to: {out_path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Interpretation of Regression Results – Predicting Additional Future Playtime**\n",
        "\n",
        "### **1. Model Purpose**\n",
        "This regression model predicts **how many minutes of additional playtime** a player will accumulate *after* writing a review, using features available at review time (playtime at review, review metadata, sentiment, lexical features, etc.).\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Main Model Performance (on Test Set)**  \n",
        "- **MAE:** **2017 minutes** (≈ **33.6 hours**)  \n",
        "- **RMSE:** **3771 minutes** (≈ **62.8 hours**)  \n",
        "\n",
        "**Meaning:**  \n",
        "On average, the model’s prediction of future additional playtime is off by **~34 hours** even after clipping extreme players (top 1%).\n",
        "\n",
        "This is expected because player behavior is **highly variable** and dominated by heavy-tail users.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Baseline Comparison (Test Set)**  \n",
        "\n",
        "| Baseline | MAE | RMSE | Interpretation |\n",
        "|---------|------|--------|----------------|\n",
        "| **Mean baseline** (predict constant mean) | **2466 min** | **3615 min** | Worst MAE, decent RMSE |\n",
        "| **Median baseline** (predict constant median) | **2125 min** | **3935 min** | Better MAE but higher RMSE |\n",
        "\n",
        "**Model vs baselines:**\n",
        "- **MAE improved** (2017 vs 2466 / 2125) → model captures useful predictive signals  \n",
        "- **RMSE mixed** → model still struggles with extreme players (heavy-tail effect)\n",
        "\n",
        "---\n",
        "\n",
        "### **4. What These Numbers Tell Us**\n",
        "1. **The model meaningfully beats naive predictions**  \n",
        "   - Review-time features help predict future engagement  \n",
        "   - Especially variables like:  \n",
        "     - playtime_at_review  \n",
        "     - sentiment_score  \n",
        "     - review length  \n",
        "     - number of owned games  \n",
        "     - votes_up / votes_funny  \n",
        "\n",
        "2. **The heavy-tail nature of Steam players dominates error**  \n",
        "   RMSE is high because a small group of players accumulate **hundreds of hours**, making them inherently difficult to predict.\n",
        "\n",
        "3. **MAE is the better metric here**  \n",
        "   Because it is less affected by extreme outliers.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Overall Conclusion (for the report)**  \n",
        "This model provides **non-trivial predictive power** for estimating a player’s future engagement after writing a review.  \n",
        "It beats simple constant baselines and proves that **review-time behavior contains meaningful signal**.\n",
        "\n",
        "In business terms, the model can support:\n",
        "- **Player retention analysis**  \n",
        "- **Churn prediction** (low future playtime → likely churn)  \n",
        "- **Personalized recommendations / interventions**  \n",
        "- **Identifying highly engaged players early**\n",
        "\n",
        "Even though exact minute-level predictions remain noisy due to behavioral complexity, the model delivers valuable insight into **who will keep playing** and **at what magnitude**.\n",
        "\n"
      ],
      "metadata": {
        "id": "Drs27BTfjEGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# =============================================================================\n",
        "# Project description (updated: binary classification version)\n",
        "# =============================================================================\n",
        "# This script trains a supervised learning model to predict whether a player\n",
        "# will increase their playtime AFTER writing a Steam review (binary outcome).\n",
        "#\n",
        "# Data:\n",
        "#   - Three Steam review datasets for specific games, stored as Excel files:\n",
        "#       /content/Cyberpunk 2077_steam_reviews_1091500.part000_partial (1).xlsx\n",
        "#       /content/Red Dead Redemption2_steam_reviews_1174180.part000_with_sent (1).xlsx\n",
        "#       /content/Witcher 3_steam_reviews_292030.part000_with_sent (1).xlsx\n",
        "#\n",
        "# Target variable (binary):\n",
        "#   - author_playtime_forever: total playtime in minutes (measured later)\n",
        "#   - author_playtime_at_review: playtime in minutes at review time\n",
        "#   - We define the binary target:\n",
        "#       target_increase = 1  if author_playtime_forever > author_playtime_at_review\n",
        "#                        0  otherwise\n",
        "#     In words: 1 if the player plays more after the review, 0 if they do not\n",
        "#     increase their total minutes (no change).\n",
        "#\n",
        "# Main modeling choices:\n",
        "#   1. We remove rows with missing playtime columns and rows where\n",
        "#      delta_playtime = author_playtime_forever - author_playtime_at_review < 0\n",
        "#      (these look like data inconsistency).\n",
        "#   2. We still compute delta_playtime as an intermediate quantity for cleaning\n",
        "#      and descriptive purposes, but the final target is binary (increase vs no\n",
        "#      increase), not a continuous number of minutes.\n",
        "#   3. We use HistGradientBoostingClassifier as the main model, with all\n",
        "#      structured features (player stats, review stats, sentiment, lexical\n",
        "#      features, language).\n",
        "#   4. We split the data into train / validation / test using GroupShuffleSplit\n",
        "#      by author_steamid, so that the same player does not appear in both\n",
        "#      train and test sets (prevents leakage).\n",
        "#   5. Input features include:\n",
        "#        - numeric player and review statistics (playtime at review,\n",
        "#          last two weeks' playtime, votes_up, votes_funny, etc.),\n",
        "#        - binary flags (voted_up, steam_purchase, etc.),\n",
        "#        - textual / lexical features (length, specificity, etc., if present),\n",
        "#        - sentiment label and sentiment score (if available),\n",
        "#        - language of the review.\n",
        "#\n",
        "# Outputs:\n",
        "#   - The script prints classification metrics on:\n",
        "#       - Training set\n",
        "#       - Validation set\n",
        "#       - Test set\n",
        "#     including:\n",
        "#       * Accuracy\n",
        "#       * F1-score (for the positive class: \"increased playtime\")\n",
        "#   - It also prints a simple baseline where we always predict the majority\n",
        "#     class on the test set and reports its Accuracy / F1-score.\n",
        "#   - Finally, it saves a CSV file with test-set rows and model predictions:\n",
        "#       /content/test_predictions_playtime_increase_binary.csv\n",
        "#     containing:\n",
        "#       - target_increase_true  (0/1, ground truth)\n",
        "#       - target_increase_pred  (0/1, model prediction)\n",
        "#       - target_increase_proba (predicted probability of increase = 1)\n",
        "#\n",
        "# How to interpret the results:\n",
        "#   - Accuracy:\n",
        "#       Proportion of reviews for which the model correctly predicts whether\n",
        "#       the player will increase their playtime after reviewing.\n",
        "#   - F1-score (positive class = \"increase\"):\n",
        "#       Harmonic mean of precision and recall for the positive class.\n",
        "#       Useful when the classes are imbalanced (e.g., many players do or do\n",
        "#       not increase playtime).\n",
        "#   - If the model’s Accuracy and F1 are noticeably better than a naive\n",
        "#     \"majority class\" baseline, it shows that review-time features (playtime\n",
        "#     at review, sentiment, lexical features, etc.) carry signal about whether\n",
        "#     players will continue to engage with the game.\n",
        "#\n",
        "# Business meaning:\n",
        "#   - This model is an \"engagement continuation\" classifier:\n",
        "#       Given what we know at review time (playtime, review text features,\n",
        "#       sentiment), can we predict whether the player will continue to play?\n",
        "#   - Platforms or developers can use it to:\n",
        "#       * Identify players likely to continue vs likely to stop,\n",
        "#       * Understand what types of reviews / user states are associated with\n",
        "#         continued engagement,\n",
        "#       * Design interventions or personalized nudges for players predicted\n",
        "#         not to continue playing.\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 0. Config\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "# Excel files uploaded to /content in Colab\n",
        "DATA_FILES = [\n",
        "    \"/content/Cyberpunk 2077_steam_reviews_1091500.part000_partial (1).xlsx\",\n",
        "    \"/content/Red Dead Redemption2_steam_reviews_1174180.part000_with_sent (1).xlsx\",\n",
        "    \"/content/Witcher 3_steam_reviews_292030.part000_with_sent (1).xlsx\",\n",
        "]\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.2\n",
        "VAL_SIZE = 0.2  # fraction of (train+val) that will be used as validation\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1. Load and concatenate data\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "def load_all_files(file_list) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load all provided Excel files and concatenate them into a single DataFrame.\n",
        "    \"\"\"\n",
        "    all_dfs = []\n",
        "\n",
        "    for path in file_list:\n",
        "        if not os.path.exists(path):\n",
        "            raise FileNotFoundError(f\"File not found: {path}\")\n",
        "        print(f\"Loading XLSX: {path}\")\n",
        "        df = pd.read_excel(path)  # Colab has openpyxl support by default\n",
        "        all_dfs.append(df)\n",
        "\n",
        "    df_all = pd.concat(all_dfs, ignore_index=True)\n",
        "    print(f\"Total rows after concat: {len(df_all)}\")\n",
        "    return df_all\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2. Basic cleaning + target construction (binary label)\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "def coalesce_column(df: pd.DataFrame, main_col: str, backup_cols):\n",
        "    \"\"\"\n",
        "    Ensure df[main_col] exists and fill its NaNs from backup_cols in order.\n",
        "    backup_cols: list of column names that may or may not exist.\n",
        "    \"\"\"\n",
        "    if main_col not in df.columns:\n",
        "        df[main_col] = np.nan\n",
        "\n",
        "    for bcol in backup_cols:\n",
        "        if bcol in df.columns:\n",
        "            df[main_col] = df[main_col].fillna(df[bcol])\n",
        "\n",
        "\n",
        "def prepare_dataset(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Prepare the dataset:\n",
        "      - coalesce ID / language / playtime columns from partial variants,\n",
        "      - clean author_steamid,\n",
        "      - convert playtime columns to numeric,\n",
        "      - construct binary target: target_increase (1 if playtime increases),\n",
        "      - build numeric and categorical feature columns used by the model.\n",
        "    \"\"\"\n",
        "    df = df_raw.copy()\n",
        "\n",
        "    # ---- 2.1 Coalesce key ID / language / playtime columns ----------------\n",
        "    coalesce_column(\n",
        "        df,\n",
        "        \"author_steamid\",\n",
        "        [\n",
        "            \"author_steamid_from_partial\",\n",
        "            \"author_steamid_from_partial_from_partial\",\n",
        "        ],\n",
        "    )\n",
        "    coalesce_column(\n",
        "        df,\n",
        "        \"author_language\",\n",
        "        [\n",
        "            \"author_language_from_partial\",\n",
        "            \"author_language_from_partial.1\",\n",
        "            \"author_language_from_partial_from_partial\",\n",
        "        ],\n",
        "    )\n",
        "    coalesce_column(\n",
        "        df,\n",
        "        \"author_playtime_at_review\",\n",
        "        [\n",
        "            \"author_playtime_at_review_from_partial\",\n",
        "            \"author_playtime_at_review_from_partial.1\",\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    # Ensure author_steamid exists and is usable for grouping\n",
        "    if \"author_steamid\" not in df.columns:\n",
        "        raise ValueError(\"Column 'author_steamid' is required but not found.\")\n",
        "    df[\"author_steamid\"] = df[\"author_steamid\"].astype(str).str.strip()\n",
        "\n",
        "    # Drop rows with missing author_steamid\n",
        "    before_auth = len(df)\n",
        "    df = df.replace({\"\": np.nan})\n",
        "    df = df.dropna(subset=[\"author_steamid\"])\n",
        "    after_auth = len(df)\n",
        "    print(f\"Dropped {before_auth - after_auth} rows with missing author_steamid; remaining: {after_auth}\")\n",
        "\n",
        "    # ---- 2.2 Numeric conversion for playtime columns ----------------------\n",
        "    for col in [\n",
        "        \"author_playtime_forever\",\n",
        "        \"author_playtime_at_review\",\n",
        "        \"author_playtime_last_two_weeks\",\n",
        "    ]:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "    # ---- 2.3 Compute delta_playtime and construct binary target -----------\n",
        "    df[\"delta_playtime\"] = df[\"author_playtime_forever\"] - df[\"author_playtime_at_review\"]\n",
        "\n",
        "    before = len(df)\n",
        "    df = df.dropna(subset=[\"author_playtime_forever\", \"author_playtime_at_review\", \"delta_playtime\"])\n",
        "    # Remove obviously inconsistent rows with negative delta_playtime\n",
        "    df = df[df[\"delta_playtime\"] >= 0]\n",
        "    after = len(df)\n",
        "    print(f\"Dropped {before - after} rows with missing/negative delta_playtime; remaining: {after}\")\n",
        "\n",
        "    # Binary target: 1 if playtime increased, 0 otherwise (no increase)\n",
        "    df[\"target_increase\"] = (df[\"author_playtime_forever\"] > df[\"author_playtime_at_review\"]).astype(int)\n",
        "\n",
        "    positive_rate = df[\"target_increase\"].mean()\n",
        "    print(f\"Positive class rate (increase=1): {positive_rate:.3f}\")\n",
        "\n",
        "    # ---- 2.4 Boolean-like columns → 0/1 -----------------------------------\n",
        "    bool_cols = [\n",
        "        \"voted_up\",\n",
        "        \"steam_purchase\",\n",
        "        \"received_for_free\",\n",
        "        \"written_during_early_access\",\n",
        "        \"primarily_steam_deck\",\n",
        "    ]\n",
        "    for col in bool_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = (\n",
        "                df[col]\n",
        "                .replace(\n",
        "                    {\n",
        "                        \"TRUE\": 1,\n",
        "                        \"True\": 1,\n",
        "                        True: 1,\n",
        "                        \"FALSE\": 0,\n",
        "                        \"False\": 0,\n",
        "                        False: 0,\n",
        "                    }\n",
        "                )\n",
        "                .fillna(0)\n",
        "                .astype(int)\n",
        "            )\n",
        "        else:\n",
        "            df[col] = 0\n",
        "\n",
        "    # ---- 2.5 Sentiment & lexical features ---------------------------------\n",
        "    # These three files may or may not have sentiment_label / sentiment_score.\n",
        "    # We provide safe defaults and fill missing values.\n",
        "\n",
        "    if \"sentiment_label\" not in df.columns:\n",
        "        df[\"sentiment_label\"] = \"unknown\"\n",
        "    df[\"sentiment_label\"] = df[\"sentiment_label\"].astype(str).fillna(\"unknown\")\n",
        "\n",
        "    if \"sentiment_score\" in df.columns:\n",
        "        df[\"sentiment_score\"] = pd.to_numeric(df[\"sentiment_score\"], errors=\"coerce\").fillna(0.0)\n",
        "    else:\n",
        "        df[\"sentiment_score\"] = 0.0\n",
        "\n",
        "    # author_language\n",
        "    if \"author_language\" in df.columns:\n",
        "        df[\"author_language\"] = df[\"author_language\"].astype(str).fillna(\"unknown\")\n",
        "    else:\n",
        "        df[\"author_language\"] = \"unknown\"\n",
        "\n",
        "    # Numeric text features — used in the econ lexical dataset; safe to apply here too.\n",
        "    numeric_text_cols = [\n",
        "        \"len_char\",\n",
        "        \"len_token\",\n",
        "        \"R_len\",\n",
        "        \"R_digit\",\n",
        "        \"specificity_idf\",\n",
        "        \"R_spec\",\n",
        "        \"R_fam\",\n",
        "        \"R_conc\",\n",
        "        \"specificity_lex\",\n",
        "    ]\n",
        "    for col in numeric_text_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0.0)\n",
        "        else:\n",
        "            df[col] = 0.0\n",
        "\n",
        "    # ---- 2.6 Other numeric features for the model -------------------------\n",
        "    other_numeric_cols = [\n",
        "        \"author_num_games_owned\",\n",
        "        \"author_num_reviews\",\n",
        "        \"author_playtime_at_review\",\n",
        "        \"author_playtime_last_two_weeks\",\n",
        "        \"votes_up\",\n",
        "        \"votes_funny\",\n",
        "        \"comment_count\",\n",
        "        \"weighted_vote_score\",\n",
        "    ]\n",
        "    for col in other_numeric_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0.0)\n",
        "        else:\n",
        "            df[col] = 0.0\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3. Group-based train/val/test split by author_steamid\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "def group_train_val_test_split(\n",
        "    df: pd.DataFrame,\n",
        "    group_col: str = \"author_steamid\",\n",
        "    test_size: float = TEST_SIZE,\n",
        "    val_size: float = VAL_SIZE,\n",
        "    random_state: int = RANDOM_STATE,\n",
        "):\n",
        "    \"\"\"\n",
        "    Split the DataFrame into train / validation / test sets based on groups\n",
        "    (here: author_steamid), so that the same player does not appear in multiple\n",
        "    splits.\n",
        "    \"\"\"\n",
        "    from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "    groups = df[group_col].values\n",
        "\n",
        "    # 1) Train+val vs test\n",
        "    gss1 = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
        "    train_val_idx, test_idx = next(gss1.split(df, groups=groups))\n",
        "\n",
        "    df_train_val = df.iloc[train_val_idx].reset_index(drop=True)\n",
        "    df_test = df.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "    # 2) Train vs val inside train_val\n",
        "    groups_tv = df_train_val[group_col].values\n",
        "    gss2 = GroupShuffleSplit(n_splits=1, test_size=val_size, random_state=random_state + 1)\n",
        "    train_idx, val_idx = next(gss2.split(df_train_val, groups=groups_tv))\n",
        "\n",
        "    df_train = df_train_val.iloc[train_idx].reset_index(drop=True)\n",
        "    df_val = df_train_val.iloc[val_idx].reset_index(drop=True)\n",
        "\n",
        "    print(\n",
        "        f\"Train size: {len(df_train)}, Val size: {len(df_val)}, Test size: {len(df_test)}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"Unique authors - Train: {df_train[group_col].nunique()}, \"\n",
        "        f\"Val: {df_val[group_col].nunique()}, \"\n",
        "        f\"Test: {df_test[group_col].nunique()}\"\n",
        "    )\n",
        "\n",
        "    return df_train, df_val, df_test\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4. Build model pipeline (classifier)\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "def build_model():\n",
        "    \"\"\"\n",
        "    Define the preprocessing and classification pipeline:\n",
        "      - Scale numeric features,\n",
        "      - Pass through boolean features,\n",
        "      - One-hot encode categorical features,\n",
        "      - Train HistGradientBoostingClassifier.\n",
        "    \"\"\"\n",
        "    # Continuous numeric features\n",
        "    numeric_features = [\n",
        "        \"author_num_games_owned\",\n",
        "        \"author_num_reviews\",\n",
        "        \"author_playtime_at_review\",\n",
        "        \"author_playtime_last_two_weeks\",\n",
        "        \"votes_up\",\n",
        "        \"votes_funny\",\n",
        "        \"comment_count\",\n",
        "        \"weighted_vote_score\",\n",
        "        \"len_char\",\n",
        "        \"len_token\",\n",
        "        \"R_len\",\n",
        "        \"R_digit\",\n",
        "        \"specificity_idf\",\n",
        "        \"R_spec\",\n",
        "        \"R_fam\",\n",
        "        \"R_conc\",\n",
        "        \"specificity_lex\",\n",
        "        \"sentiment_score\",\n",
        "    ]\n",
        "\n",
        "    # Boolean / 0-1 features\n",
        "    bool_features = [\n",
        "        \"voted_up\",\n",
        "        \"steam_purchase\",\n",
        "        \"received_for_free\",\n",
        "        \"written_during_early_access\",\n",
        "        \"primarily_steam_deck\",\n",
        "    ]\n",
        "\n",
        "    # Categorical features\n",
        "    categorical_features = [\n",
        "        \"author_language\",\n",
        "        \"sentiment_label\",\n",
        "    ]\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", StandardScaler(), numeric_features),\n",
        "            (\"bool\", \"passthrough\", bool_features),\n",
        "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    classifier = HistGradientBoostingClassifier(\n",
        "        loss=\"log_loss\",\n",
        "        random_state=RANDOM_STATE,\n",
        "    )\n",
        "\n",
        "    model = Pipeline(\n",
        "        steps=[\n",
        "            (\"preprocess\", preprocessor),\n",
        "            (\"classifier\", classifier),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    feature_cols = numeric_features + bool_features + categorical_features\n",
        "    return model, feature_cols\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5. Train / evaluate (classification)\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "def evaluate_classification(y_true, y_pred, split_name: str):\n",
        "    \"\"\"\n",
        "    Print classification metrics:\n",
        "      - Accuracy\n",
        "      - F1-score for the positive class (label=1)\n",
        "    \"\"\"\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, pos_label=1)\n",
        "    print(f\"[{split_name}] Accuracy = {acc:.4f}, F1 (increase=1) = {f1:.4f}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    # 1) Load all data files\n",
        "    df_raw = load_all_files(DATA_FILES)\n",
        "\n",
        "    # 2) Clean data and construct the binary target\n",
        "    df = prepare_dataset(df_raw)\n",
        "\n",
        "    # 3) Split by author_steamid (group-based)\n",
        "    df_train, df_val, df_test = group_train_val_test_split(df, group_col=\"author_steamid\")\n",
        "\n",
        "    # 4) Build model and feature list\n",
        "    model, feature_cols = build_model()\n",
        "\n",
        "    # 5) Prepare X / y  (target = target_increase)\n",
        "    X_train = df_train[feature_cols]\n",
        "    y_train = df_train[\"target_increase\"].astype(int)\n",
        "\n",
        "    X_val = df_val[feature_cols]\n",
        "    y_val = df_val[\"target_increase\"].astype(int)\n",
        "\n",
        "    X_test = df_test[feature_cols]\n",
        "    y_test = df_test[\"target_increase\"].astype(int)\n",
        "\n",
        "    # 6) Fit the model\n",
        "    print(\"\\nFitting model (binary classification: playtime increased vs not) ...\\n\")\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # 7) Predict labels and probabilities\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_val_pred = model.predict(X_val)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    # Probability of the positive class (increase=1)\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_test_proba = model.predict_proba(X_test)[:, 1]\n",
        "    else:\n",
        "        # Fallback: no probability interface; use zeros\n",
        "        y_test_proba = np.zeros_like(y_test_pred, dtype=float)\n",
        "\n",
        "    # 8) Evaluation\n",
        "    print(\"Evaluation on binary target (increase=1, no increase=0):\")\n",
        "    evaluate_classification(y_train, y_train_pred, \"Train\")\n",
        "    evaluate_classification(y_val, y_val_pred, \"Validation\")\n",
        "    evaluate_classification(y_test, y_test_pred, \"Test\")\n",
        "\n",
        "    print(\"\\nDetailed classification report on Test set:\")\n",
        "    print(classification_report(y_test, y_test_pred, digits=4))\n",
        "\n",
        "    print(\"Confusion matrix on Test set (rows=true, cols=pred):\")\n",
        "    print(confusion_matrix(y_test, y_test_pred))\n",
        "\n",
        "    # 9) Baseline: majority class on Test\n",
        "    print(\"\\nBaseline on Test (predicting the majority class):\")\n",
        "    majority_label = int(round(y_test.mean()))  # 1 if positive rate >= 0.5, else 0\n",
        "    y_test_baseline = np.full_like(y_test, majority_label)\n",
        "\n",
        "    evaluate_classification(y_test, y_test_baseline, \"Test (majority baseline)\")\n",
        "    print(f\"Majority label used by baseline: {majority_label} \"\n",
        "          f\"(positive rate in test = {y_test.mean():.3f})\")\n",
        "\n",
        "    # 10) Save test predictions\n",
        "    df_test_out = df_test.copy()\n",
        "    df_test_out[\"target_increase_true\"] = y_test\n",
        "    df_test_out[\"target_increase_pred\"] = y_test_pred\n",
        "    df_test_out[\"target_increase_proba\"] = y_test_proba\n",
        "\n",
        "    out_path = \"/content/test_predictions_playtime_increase_binary.csv\"\n",
        "    df_test_out.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
        "    print(f\"\\nSaved test predictions to: {out_path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "oB2PJAECjgSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Binary Classification Results – Future Playtime Increase**\n",
        "\n",
        "### **1. Task & Data**\n",
        "- Target: **whether future playtime increased** after the review  \n",
        "  - `1` = total playtime after review > playtime at review  \n",
        "  - `0` = no increase  \n",
        "- Total reviews: **107,874**  \n",
        "- Positive class rate (`increase = 1`): **83.2%** → **strongly imbalanced** dataset\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Overall Model Performance (Test Set)**\n",
        "\n",
        "- **Accuracy:** **0.8364**  \n",
        "- **F1-score (increase = 1):** **0.9092**\n",
        "\n",
        "The model is very good at identifying players whose playtime **increases** after writing a review.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Class-wise Performance (Test Set)**\n",
        "\n",
        "- **Class 1 – Increase**\n",
        "  - Precision: **0.8436**\n",
        "  - Recall: **0.9858**\n",
        "  - F1-score: **0.9092**\n",
        "- **Class 0 – No Increase**\n",
        "  - Precision: **0.5939**\n",
        "  - Recall: **0.1021**\n",
        "  - F1-score: **0.1742**\n",
        "\n",
        "**Interpretation:**  \n",
        "- The model **almost always** detects players who will keep playing (very high recall for class 1).  \n",
        "- It **struggles to detect non-increase players** (very low recall for class 0), which is typical in imbalanced data.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Confusion Matrix (Test Set)**\n",
        "\n",
        "- True 0, Pred 0: **373**  \n",
        "- True 0, Pred 1: **3,281**  \n",
        "- True 1, Pred 0: **255**  \n",
        "- True 1, Pred 1: **17,701**\n",
        "\n",
        "Most mistakes come from predicting **“increase”** when there is actually **no increase**.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Comparison with Majority-Class Baseline**\n",
        "\n",
        "- **Majority baseline (always predict 1):**\n",
        "  - Accuracy: **0.8309**\n",
        "  - F1 (increase=1): **0.9076**\n",
        "- **Our model:**\n",
        "  - Accuracy: **0.8364**\n",
        "  - F1 (increase=1): **0.9092**\n",
        "\n",
        "➡️ The model **outperforms** the naive majority baseline, showing that **review-time features (playtime, sentiment, text stats, etc.) add real predictive value.**\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Takeaway for the Project**\n",
        "\n",
        "- The classifier is **reliable for detecting engaged players** who will continue playing.  \n",
        "- It is **less reliable for detecting potential churners** (no future increase).  \n",
        "- For this final project, the results demonstrate:\n",
        "  - **Non-trivial predictive power** beyond a simple baseline  \n",
        "  - A realistic example of **class imbalance** and its impact on model behavior\n"
      ],
      "metadata": {
        "id": "biATYmMqmQyz"
      }
    }
  ]
}